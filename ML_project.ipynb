{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charithm-7/FOOD_101/blob/main/ML_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUsXBXW9riPt",
        "outputId": "1b299299-8160-4343-ed75-e7def5327c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading the data...\n",
            "--2024-10-21 15:26:04--  http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz [following]\n",
            "--2024-10-21 15:26:05--  https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4996278331 (4.7G) [application/x-gzip]\n",
            "Saving to: ‘food-101.tar.gz’\n",
            "\n",
            "food-101.tar.gz     100%[===================>]   4.65G  22.4MB/s    in 3m 38s  \n",
            "\n",
            "2024-10-21 15:29:43 (21.9 MB/s) - ‘food-101.tar.gz’ saved [4996278331/4996278331]\n",
            "\n",
            "Dataset downloaded!\n",
            "Extracting data..\n",
            "Extraction done!\n",
            "Testing images\n",
            "apple_pie/1011328\n",
            "apple_pie/101251\n",
            "apple_pie/1034399\n",
            "apple_pie/103801\n",
            "apple_pie/1038694\n",
            "\n",
            "Training images\n",
            "apple_pie/1005649\n",
            "apple_pie/1014775\n",
            "apple_pie/1026328\n",
            "apple_pie/1028787\n",
            "apple_pie/1043283\n",
            "apple_pie 0\n",
            "baby_back_ribs 1\n",
            "baklava 2\n",
            "beef_carpaccio 3\n",
            "beef_tartare 4\n",
            "beet_salad 5\n",
            "beignets 6\n",
            "bibimbap 7\n",
            "bread_pudding 8\n",
            "breakfast_burrito 9\n",
            "bruschetta 10\n",
            "caesar_salad 11\n",
            "cannoli 12\n",
            "caprese_salad 13\n",
            "carrot_cake 14\n",
            "ceviche 15\n",
            "cheesecake 16\n",
            "cheese_plate 17\n",
            "chicken_curry 18\n",
            "chicken_quesadilla 19\n",
            "seaweed_salad torch.Size([3, 224, 224])\n",
            "hot_dog torch.Size([3, 224, 224])\n",
            "french_fries torch.Size([3, 224, 224])\n",
            "lobster_roll_sandwich torch.Size([3, 224, 224])\n",
            "huevos_rancheros torch.Size([3, 224, 224])\n",
            "hummus torch.Size([3, 224, 224])\n",
            "pho torch.Size([3, 224, 224])\n",
            "deviled_eggs torch.Size([3, 224, 224])\n",
            "omelette torch.Size([3, 224, 224])\n",
            "red_velvet_cake torch.Size([3, 224, 224])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n",
            "100%|██████████| 77.4M/77.4M [00:00<00:00, 101MB/s]\n",
            "<ipython-input-1-e7ef65e100e4>:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path,map_location='cpu'),strict=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "--> Training Progress\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 75/592 [1:24:05<9:54:09, 68.96s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "from sklearn.utils import shuffle\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "from collections import OrderedDict\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n",
        "\n",
        "import os\n",
        "print(os.getcwd())\n",
        "if \"food-101\" in os.listdir():\n",
        "    print(\"Dataset already exists\")\n",
        "else:\n",
        "    print(\"Downloading the data...\")\n",
        "    !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "    print(\"Dataset downloaded!\")\n",
        "    print(\"Extracting data..\")\n",
        "    !tar xzvf food-101.tar.gz > /dev/null 2>&1\n",
        "    print(\"Extraction done!\")\n",
        "classes = open(\"./food-101/meta/classes.txt\", 'r').read().splitlines()\n",
        "classes_21 = classes[:20] + ['other']\n",
        "classes_21, len(classes_21)\n",
        "\n",
        "!echo \"Testing images\"\n",
        "!head -n 5 ./food-101/meta/test.txt\n",
        "!echo -e \"\\nTraining images\"\n",
        "!head -n 5 ./food-101/meta/train.txt | head -n 5\n",
        "\n",
        "def prep_df(path: str) -> pd.DataFrame:\n",
        "    array = open(path, 'r').read().splitlines()\n",
        "\n",
        "    # Getting the full path for the images\n",
        "    img_path = \"./food-101/images/\"\n",
        "    full_path = [img_path + img + \".jpg\" for img in array]\n",
        "\n",
        "    # Splitting the image index from the label\n",
        "    imgs = []\n",
        "    for img in array:\n",
        "        img = img.split('/')\n",
        "\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.array(imgs)\n",
        "    # Converting the array to a data frame\n",
        "    imgs = pd.DataFrame(imgs[:,0], imgs[:,1], columns=['label'])\n",
        "    # Adding the full path to the data frame\n",
        "    imgs['path'] = full_path\n",
        "\n",
        "    # Randomly shuffling the order to the data in the dataframe\n",
        "    imgs = shuffle(imgs)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "train_imgs = prep_df('./food-101/meta/train.txt')\n",
        "test_imgs = prep_df('./food-101/meta/test.txt')\n",
        "\n",
        "train_imgs.head(5)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "num_rows = 3\n",
        "num_cols = 8\n",
        "\n",
        "\n",
        "for idx in range(num_rows * num_cols):\n",
        "    random_idx = np.random.randint(0, train_imgs.shape[0])\n",
        "    img = plt.imread(train_imgs.path.iloc[random_idx])\n",
        "\n",
        "    label = train_imgs.label.iloc[random_idx]\n",
        "\n",
        "    ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(label)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "\n",
        "# Data augmentation for training\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       torchvision.transforms.AutoAugment(torchvision.transforms.AutoAugmentPolicy.IMAGENET),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "# Data augmentation for testing\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "class Label_encoder:\n",
        "    def __init__(self, labels):\n",
        "        labels = list(set(labels))\n",
        "        self.labels = {label: idx for idx, label in enumerate(classes)}\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        return list(self.labels.keys())[idx]\n",
        "\n",
        "    def get_idx(self, label):\n",
        "        return self.labels[label]\n",
        "\n",
        "encoder = Label_encoder(classes)\n",
        "for i in range(20):\n",
        "    print(encoder.get_label(i), encoder.get_idx( encoder.get_label(i) ))\n",
        "\n",
        "class Food20(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataframe.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.path.iloc[idx]\n",
        "        image = Image.open(img_name)\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        label = encoder.get_idx(self.dataframe.label.iloc[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "train_dataset = Food20(train_imgs, transform=train_transforms)\n",
        "test_dataset = Food20(test_imgs, transform=test_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "# Testing the retrieval of a single image\n",
        "for i in range(10):\n",
        "    image = train_dataset.__getitem__(i)\n",
        "    print(encoder.get_label(image[1]), image[0].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "weights = models.DenseNet201_Weights.IMAGENET1K_V1\n",
        "model = models.densenet201(weights = weights)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "import requests as reqs\n",
        "\n",
        "url = \"https://github.com/Prakhar998/Food-Classification/raw/master/food_classifier.pt\"\n",
        "r = reqs.get(url, allow_redirects=True)\n",
        "\n",
        "open(\"./food_classifier.pt\", \"wb\").write(r.content)\n",
        "\n",
        "\n",
        "checkpoint_path = \"./food_classifier.pt\"\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(1920,1024),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(1024,101),\n",
        ")\n",
        "\n",
        "model.classifier = classifier\n",
        "model.load_state_dict(torch.load(checkpoint_path,map_location='cpu'),strict=False)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#hyper parameters\n",
        "num_epochs = 3\n",
        "\n",
        "# loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=[0.9, 0.999])\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  print(\"--> Training Progress\")\n",
        "  # Loop through data loader data batches\n",
        "  for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "      # Send data to target device\n",
        "      images, labels = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(images)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, labels)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == labels).sum().item()/len(y_pred)\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device):\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "      print(\"--> Testing Progress\")\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "          # Send data to target device\n",
        "          images, labels = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(images)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, labels)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim=1), dim=1)\n",
        "\n",
        "          test_acc += ((test_pred_labels == labels).sum().item()/len(test_pred_labels))\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device):\n",
        "  # Create empty results dictionary\n",
        "  history = {\n",
        "      \"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "      'best train acc': (0, 0),\n",
        "      \"best_model\": dict()\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in range(epochs):\n",
        "      print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
        "\n",
        "      train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "      test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "          f\"\\n\\n=============================\\n\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      history[\"train_loss\"].append(train_loss)\n",
        "      history[\"train_acc\"].append(train_acc)\n",
        "      history[\"test_loss\"].append(test_loss)\n",
        "      history[\"test_acc\"].append(test_acc)\n",
        "      if test_loss < history[\"test_acc\"][len(history[\"test_acc\"]) - 1]:\n",
        "          history[\"best_model\"] = model.state_dict()\n",
        "\n",
        "      if test_acc > 0.95:\n",
        "         break\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return model, history\n",
        "model, history = train(model, train_loader, test_loader, optimizer, loss_fn, num_epochs, device)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "\n",
        "  random = np.random.randint(0, len(dataloader))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for images, labels in tqdm(dataloader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      preds = torch.argmax(torch.softmax(outputs, 1), 1)\n",
        "\n",
        "      # Converting this problem to a problem with 21 clases only\n",
        "      preds = np.array([pred.cpu() if pred < 20 else 20 for pred in preds])\n",
        "      labels = np.array([label.cpu() if label < 20 else 20 for label in labels])\n",
        "\n",
        "      n_samples += labels.shape[0]\n",
        "      n_correct += (preds==labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(acc)\n",
        "evaluate(model,test_loader)\n",
        "\n",
        "class Label_encoder_21:\n",
        "    def __init__(self, labels):\n",
        "        labels = list(set(labels))\n",
        "        self.labels = {label: idx for idx, label in enumerate(labels)}\n",
        "\n",
        "    def get_label(self, idx):\n",
        "        return list(self.labels.keys())[idx]\n",
        "\n",
        "    def get_idx(self, label):\n",
        "        return self.labels[label]\n",
        "\n",
        "encoder_21 = Label_encoder(classes_21)\n",
        "encoder_21.get_label(0), encoder.get_idx( encoder_21.get_label(0) )\n",
        "\n",
        "#This line of code saves the best model's state dictionary (or parameters) from the training history to a file named solution.pth.\n",
        "torch.save(history['best_model'], \"./solution.pth\")\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./solution.pth\"):\n",
        "    print(\"solution.pth exists in the current directory.\")\n",
        "else:\n",
        "    print(\"solution.pth does not exist in the current directory.\")\n",
        "\n",
        "torch.save(model.state_dict(), 'saved_model.pth')\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def classify_image(image_path, model, label_encoder, device):\n",
        "    # Load and preprocess the input image\n",
        "    image = Image.open(image_path)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Perform prediction\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        output = model(image_tensor)\n",
        "\n",
        "    # Get predicted class index\n",
        "    _, predicted_idx = torch.max(output, 1)\n",
        "    predicted_idx = predicted_idx.item()\n",
        "\n",
        "    # Map index to class name\n",
        "    predicted_label = label_encoder.get_label(predicted_idx)\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Load the saved model and label encoder\n",
        "model = models.densenet201(weights=None)\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(1920, 1024),\n",
        "    nn.LeakyReLU(),\n",
        "    nn.Linear(1024, 101),\n",
        ")\n",
        "model.classifier = classifier\n",
        "model.load_state_dict(torch.load(\"solution.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "label_encoder = Label_encoder(classes)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the model file exists\n",
        "model_path = \"solution.pth\"\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(f\"Error: Model file '{model_path}' not found.\")\n",
        "    # Initialize weights using a suitable method for Conv2d layers\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                torch.nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            torch.nn.init.constant_(m.weight, 1)\n",
        "            torch.nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            torch.nn.init.normal_(m.weight, 0, 0.01)\n",
        "            torch.nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwWTUjPCeD2V",
        "outputId": "03ea3589-006d-4d37-9ded-c559a299a2fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Model file 'solution.pth' not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "def generate_confusion_matrix(model, dataloader, label_encoder, device):\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for images, labels in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(torch.softmax(outputs, dim=1), dim=1)\n",
        "\n",
        "            # Convert predictions and labels for 21-class classification\n",
        "            preds = np.array([pred.cpu() if pred < 20 else 20 for pred in preds])\n",
        "            labels = np.array([label.cpu() if label < 20 else 20 for label in labels])\n",
        "\n",
        "            true_labels.extend(labels)\n",
        "            pred_labels.extend(preds)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=np.arange(len(classes_21)))\n",
        "    return cm\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = generate_confusion_matrix(model, test_loader, encoder_21, device)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes_21)\n",
        "disp.plot(cmap='viridis', xticks_rotation='vertical', figsize=(10, 10))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xztpkF2xeFJR",
        "outputId": "3a44462b-c3ad-463b-d9a7-3518da98aedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9c4d704b3b06>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Generate the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Visualize the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9duOLV7ghVoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}